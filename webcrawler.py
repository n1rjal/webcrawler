# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YqqzzIPSdZJn3ZbpTICh-E1zIscMnda-
"""

from bs4 import BeautifulSoup
import requests
import threading
import pandas as pd
import time

global title_list,link_list
title_list=[]
link_list=[]


global counter;counter=0

def getlinks(initial_link):
  global counter
  
  if counter<15:
    headers={}  
    
    headers['User-Agent'] = 'Mozilla/5.0 (X11; Li5nux i686) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.27 Safari/537.17'
    water=requests.get(initial_link,headers=headers)
    masala=water.content
    soup=BeautifulSoup(masala,"html.parser")
    links=soup.find_all("a")
    
     
    temp_link=""
    temp_title=""

    for link in links:
      #for none types
      try:
        temp_title=link.string.strip()
      except:
        pass

      #for none types
      try:
        temp_link=link.get("href").strip()
      except:
        pass

      #discarding invalid links 
      #starting with #
      #and each link we want must start from "https://"
      #each link page must have a title
      #and when counter is not equal to 10
      #will work 
      if ((not temp_link.startswith("#") and ( temp_link.startswith("https://"))) and temp_title!="" and counter<15 and temp_link!=""):
        
        threading.Thread(name="linkfinder",daemon=True,args=([temp_link]),target=getlinks).start()        
        title_list.append(temp_title)
        link_list.append(temp_link)

    print("next level reached")
    counter+=1
    storeintxt(title_list,link_list)
  else:
    raise SystemExit

def storeintxt(title_list,link_list):

  #as is not included in links we can use it to break links later as a list
  #we can later use pandas.writecsv for the work
  #pandas is fast
  #print("wriitng in database ")
  #print ("[{}x{}]".format(len(title_list),len(link_list)))
  try:
    datas=pd.DataFrame({
      "title":title_list,
      "link":link_list
    })

    datas.to_csv("weblinks.csv",encoding="utf=8")
  except:
    pass
    
threading.Thread(name="linkfinder",daemon=False,args=(["https://www.google.com.np/search?q=football"]),target=getlinks).start()

time.sleep(360)